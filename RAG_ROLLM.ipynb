{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:15:27.184562Z",
     "iopub.status.busy": "2025-01-25T11:15:27.184344Z",
     "iopub.status.idle": "2025-01-25T11:16:04.814270Z",
     "shell.execute_reply": "2025-01-25T11:16:04.813115Z",
     "shell.execute_reply.started": "2025-01-25T11:15:27.184541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-community bitsandbytes chromadb datasets unidecode evaluate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:25.206335Z",
     "iopub.status.busy": "2025-01-25T11:16:25.205973Z",
     "iopub.status.idle": "2025-01-25T11:16:47.484657Z",
     "shell.execute_reply": "2025-01-25T11:16:47.484023Z",
     "shell.execute_reply.started": "2025-01-25T11:16:25.206303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from datasets import load_dataset\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "from evaluate import load\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading txt files using TextLoader from LangChain to create Document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:56.336936Z",
     "iopub.status.busy": "2025-01-25T11:16:56.336620Z",
     "iopub.status.idle": "2025-01-25T11:16:56.341824Z",
     "shell.execute_reply": "2025-01-25T11:16:56.340913Z",
     "shell.execute_reply.started": "2025-01-25T11:16:56.336912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(path_dir_data):\n",
    "    loaded_docs = []\n",
    "    for entry in os.scandir(path_dir_data):\n",
    "        file_path = entry.path\n",
    "        file_extension = file_path.split('.')[1]\n",
    "\n",
    "        if file_extension == 'txt':\n",
    "            loader = TextLoader(file_path)\n",
    "            document = loader.load()\n",
    "            loaded_docs.append(document)\n",
    "    \n",
    "    return loaded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the average and the maximum length of a sentence to get an overview for the size of chunk size and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:58.958911Z",
     "iopub.status.busy": "2025-01-25T11:16:58.958575Z",
     "iopub.status.idle": "2025-01-25T11:16:58.964366Z",
     "shell.execute_reply": "2025-01-25T11:16:58.963500Z",
     "shell.execute_reply.started": "2025-01-25T11:16:58.958880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def obtain_avg_max_len(path_dir_data):\n",
    "    counter_sentences = 0\n",
    "    sum_length_sentences = 0\n",
    "    max_length_sentence = 0\n",
    "\n",
    "    for entry in os.scandir(path_dir_data):\n",
    "        file_path = entry.path\n",
    "        file_extension = file_path.split('.')[1]\n",
    "\n",
    "        if file_extension == 'txt':\n",
    "            with open(file_path, 'r') as file:\n",
    "                sentences = file.read().split('.')\n",
    "                counter_sentences += len(sentences)\n",
    "                sum_length_sentences += sum(len(sentence) for sentence in sentences)\n",
    "                max_length = max(len(sentence) for sentence in sentences)\n",
    "\n",
    "                if max_length > max_length_sentence:\n",
    "                    max_length_sentence = max_length\n",
    "\n",
    "    avg_length = sum_length_sentences // counter_sentences\n",
    "    return max_length_sentence, avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:01.687606Z",
     "iopub.status.busy": "2025-01-25T11:17:01.687335Z",
     "iopub.status.idle": "2025-01-25T11:17:01.735117Z",
     "shell.execute_reply": "2025-01-25T11:17:01.734470Z",
     "shell.execute_reply.started": "2025-01-25T11:17:01.687585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_len, avg_len = obtain_avg_max_len('/kaggle/input/pentrurag')\n",
    "max_len, avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the text using RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:03.330865Z",
     "iopub.status.busy": "2025-01-25T11:17:03.330530Z",
     "iopub.status.idle": "2025-01-25T11:17:03.335196Z",
     "shell.execute_reply": "2025-01-25T11:17:03.334247Z",
     "shell.execute_reply.started": "2025-01-25T11:17:03.330838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def recursive_char_splitter(loaded_docs, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size = chunk_size,\n",
    "      chunk_overlap = chunk_overlap\n",
    "  )\n",
    "\n",
    "    return [chunk for doc in loaded_docs for chunk in text_splitter.split_documents(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query preprocessing (not that many techniques due to the fact that the texts come from a journalistic source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:05.649774Z",
     "iopub.status.busy": "2025-01-25T11:17:05.649471Z",
     "iopub.status.idle": "2025-01-25T11:17:05.654927Z",
     "shell.execute_reply": "2025-01-25T11:17:05.653973Z",
     "shell.execute_reply.started": "2025-01-25T11:17:05.649751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QueryPreprocessor:\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "\n",
    "    def lowercase_query(self):\n",
    "        self.query = self.query.lower()\n",
    "\n",
    "    def remove_diacritics_from_query(self):\n",
    "        self.query = unidecode(self.query)\n",
    "\n",
    "    def remove_special_characters(self):\n",
    "        self.query = re.sub(r'[^ A-Za-z0-9/]+', '', self.query)\n",
    "\n",
    "    def apply_all_techniques(self):\n",
    "        self.lowercase_query()\n",
    "        self.remove_diacritics_from_query()\n",
    "        self.remove_special_characters()\n",
    "        return self.query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing to load the data for retrieval, the BASELINE was used for an initial experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:21.591820Z",
     "iopub.status.busy": "2025-01-25T11:17:21.591543Z",
     "iopub.status.idle": "2025-01-25T11:17:21.938848Z",
     "shell.execute_reply": "2025-01-25T11:17:21.937976Z",
     "shell.execute_reply.started": "2025-01-25T11:17:21.591798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loaded_docs = load_data('/kaggle/input/pentrurag')\n",
    "ratio_overlap = int(0.2 * max_len)\n",
    "split_documents_list = recursive_char_splitter(loaded_docs, 256, ratio_overlap)\n",
    "# BASELINE: split_documents_list = recursive_char_splitter(loaded_docs, 50, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the vector database and the embeddings, again the BASELINE was used for an initial experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:39.028024Z",
     "iopub.status.busy": "2025-01-25T11:17:39.027722Z",
     "iopub.status.idle": "2025-01-25T11:17:46.446317Z",
     "shell.execute_reply": "2025-01-25T11:17:46.445514Z",
     "shell.execute_reply.started": "2025-01-25T11:17:39.027999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_database = Chroma.from_documents(documents=split_documents_list, embedding=embeddings)\n",
    "retriever_model = vector_database.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.3})\n",
    "# BASELINE: retriever_model = vector_database.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the document embeddings to get a better understanding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:17:56.115700Z",
     "iopub.status.busy": "2025-01-25T11:17:56.115379Z",
     "iopub.status.idle": "2025-01-25T11:17:56.478516Z",
     "shell.execute_reply": "2025-01-25T11:17:56.477535Z",
     "shell.execute_reply.started": "2025-01-25T11:17:56.115673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(doc_embeddings):\n",
    "    pca = PCA(n_components = 2)\n",
    "    reduced_dim_embeddings = pca.fit_transform(doc_embeddings)\n",
    "\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    plt.scatter(reduced_dim_embeddings[:, 0], reduced_dim_embeddings[:, 1], c='blue', alpha=0.7)\n",
    "    plt.title('Embeddings visualization 2D')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "doc_embeddings = vector_database._collection.get(include=['embeddings'])['embeddings']\n",
    "plot_embeddings(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing a manually defined golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:18:10.984113Z",
     "iopub.status.busy": "2025-01-25T11:18:10.983694Z",
     "iopub.status.idle": "2025-01-25T11:18:10.989571Z",
     "shell.execute_reply": "2025-01-25T11:18:10.988479Z",
     "shell.execute_reply.started": "2025-01-25T11:18:10.984077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_golden_standard(golden_standard):\n",
    "    golden_standard_preprocessed = {}\n",
    "    for query, answer in golden_standard.items():\n",
    "        preprocessor = QueryPreprocessor(query)\n",
    "        query_preprocessed = preprocessor.apply_all_techniques()\n",
    "        golden_standard_preprocessed[query_preprocessed] = answer\n",
    "\n",
    "    return golden_standard_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the relevant files from the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:18:12.842207Z",
     "iopub.status.busy": "2025-01-25T11:18:12.841867Z",
     "iopub.status.idle": "2025-01-25T11:18:12.846374Z",
     "shell.execute_reply": "2025-01-25T11:18:12.845518Z",
     "shell.execute_reply.started": "2025-01-25T11:18:12.842183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def obtain_relevant_files(query_model):\n",
    "    context = retriever_model.get_relevant_documents(query_model)\n",
    "    list_files = []\n",
    "    for item in context:\n",
    "        context_file_source = item.metadata['source']\n",
    "        if context_file_source not in list_files:\n",
    "            list_files.append(context_file_source)\n",
    "\n",
    "    return list_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the retriever with the precision recall and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:18:14.617175Z",
     "iopub.status.busy": "2025-01-25T11:18:14.616861Z",
     "iopub.status.idle": "2025-01-25T11:18:14.816846Z",
     "shell.execute_reply": "2025-01-25T11:18:14.815894Z",
     "shell.execute_reply.started": "2025-01-25T11:18:14.617148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision_recall_for_retrieval(ground_truth, retrieved_files, query_model):\n",
    "    true_positives = [file for file in retrieved_files if file in ground_truth]\n",
    "    false_positives = [file for file in retrieved_files if file not in ground_truth]\n",
    "    false_negatives = [file for file in ground_truth if file not in retrieved_files]\n",
    "\n",
    "    if len(retrieved_files) == 0 or len(true_positives) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    else:\n",
    "        precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "        recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "        f1 = 2*(precision * recall) / (precision + recall)\n",
    "    \n",
    "        return [precision, recall, f1]\n",
    "\n",
    "ground_truth_files = {\n",
    "    'Ce vârstă are Anamaria Federica Oana?' : ['/kaggle/input/pentrurag/file_3.txt'],\n",
    "    'Cine este liderul campionatului de fotbal al Italiei?' : ['/kaggle/input/pentrurag/file_2 (1).txt'],\n",
    "    'Cine este Lewis Hamilton?' : ['/kaggle/input/pentrurag/file_4.txt', '/kaggle/input/pentrurag/file_15.txt'],\n",
    "    'Cine s-a confruntat pe Goodison Park?' : ['/kaggle/input/pentrurag/file_12.txt'],\n",
    "    'Împotriva cui a debutat Rapid în noul an?' : ['/kaggle/input/pentrurag/file_14.txt'],\n",
    "    'Cu ce notă a fost notat Andrei Rațiu de Sofascore?' : ['/kaggle/input/pentrurag/file_6.txt'],\n",
    "    'Pe ce loc a terminat Unirea Urziceni Grupa G în sezonul 2009/10?' : ['/kaggle/input/pentrurag/file_9.txt'],\n",
    "    'Cat va plăti Zenit St. Petersburg pentru Luiz Henrique?' : ['/kaggle/input/pentrurag/file_1 (1).txt'],\n",
    "    'Câte victorii a strâns Tag Heuer din postura de sponsor principal?' : ['/kaggle/input/pentrurag/file_10.txt'],\n",
    "    'Ce ar vrea PSG?' : ['/kaggle/input/pentrurag/file_5.txt'],\n",
    "    'De ce spune Ralf Schumacher că Lewis Hamilton nu va avea impact la Ferrari?' : ['/kaggle/input/pentrurag/file_15.txt'],\n",
    "    'Ce vârstă are Cahill, fostul antrenor al Simonei Halep?' : ['/kaggle/input/pentrurag/file_13.txt'],\n",
    "    'Cum era pe vremuri jurnalismul?' : ['/kaggle/input/pentrurag/file_11.txt'],\n",
    "    'Ce record a stabilit Andreas Almgren?' : ['/kaggle/input/pentrurag/file_7.txt'],\n",
    "    'Ce naționalitate are Beatrice Chebet?' : ['/kaggle/input/pentrurag/file_8.txt'],\n",
    "    'Spune-mi despre Radu Drăgușin' : ['/kaggle/input/pentrurag/file_12.txt', '/kaggle/input/pentrurag/file_5.txt']\n",
    "}\n",
    "sum_prec = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "\n",
    "# ground_truth_files = preprocess_golden_standard(ground_truth_files)\n",
    "\n",
    "\n",
    "for query in ground_truth_files:\n",
    "    relevant_docs = ground_truth_files[query]\n",
    "    retrieved_docs = obtain_relevant_files(query)\n",
    "    \n",
    "    metrics = calculate_precision_recall_for_retrieval(relevant_docs, retrieved_docs, query)\n",
    "    \n",
    "    \n",
    "    sum_prec += metrics[0]\n",
    "    sum_recall += metrics[1]\n",
    "    sum_f1 += metrics[2]\n",
    "    \n",
    "print(sum_prec / len(ground_truth_files))\n",
    "print(sum_recall / len(ground_truth_files))\n",
    "print(sum_f1 / len(ground_truth_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:18:53.117855Z",
     "iopub.status.busy": "2025-01-25T11:18:53.117391Z",
     "iopub.status.idle": "2025-01-25T11:24:35.362343Z",
     "shell.execute_reply": "2025-01-25T11:24:35.361615Z",
     "shell.execute_reply.started": "2025-01-25T11:18:53.117814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"OpenLLM-Ro/RoLlama2-7b-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:24:49.478550Z",
     "iopub.status.busy": "2025-01-25T11:24:49.478225Z",
     "iopub.status.idle": "2025-01-25T11:24:59.707318Z",
     "shell.execute_reply": "2025-01-25T11:24:59.706585Z",
     "shell.execute_reply.started": "2025-01-25T11:24:49.478523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rag_testing(query_model, temp, max_len):\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    context = retriever_model.get_relevant_documents(query_model)\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\"Ești un asistent folositor, respectuos și onest. Te rog să răspunzi strict pe baza informațiilor oferite în Context. \n",
    "Dacă informațiile din Context nu sunt suficiente pentru a răspunde la întrebare sau răspunsul nu se află EXPLICIT în Context, spune clar acest lucru fără a inventa răspunsuri.\"\n",
    "Question: {question}\n",
    "Context: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    prompt = prompt_template.format(question=query_model, context=context)\n",
    "\n",
    "    pipeline_generated_answer = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_len,\n",
    "        eos_token_id=terminators,\n",
    "        top_p = 0.9\n",
    "    )\n",
    "\n",
    "    llm_answer = pipeline_generated_answer(prompt)[0]['generated_text'].strip()\n",
    "\n",
    "    return llm_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:14:24.471266Z",
     "iopub.status.busy": "2025-01-25T11:14:24.470933Z",
     "iopub.status.idle": "2025-01-25T11:14:24.476288Z",
     "shell.execute_reply": "2025-01-25T11:14:24.475325Z",
     "shell.execute_reply.started": "2025-01-25T11:14:24.471243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "golden_standard = {\n",
    "    'Ce vârstă are Anamaria Federica Oana?' : '17 ani.',\n",
    "    'Cine este liderul campionatului de fotbal al Italiei?' : 'Napoli este liderul campionatului de fotbal al Italiei.',\n",
    "    'Cine este Lewis Hamilton?' : 'Lewis Hamilton este un pilot de curse britanic care a câștigat șapte campionate mondiale de Formula 1, fiind cel mai de succes pilot din istoria sportului. El este cunoscut pentru stilul său de conducere agresiv și pentru abilitatea sa de a câștiga curse în condiții dificile.',\n",
    "    'Cine s-a confruntat pe Goodison Park?' : 'Everton și Tottenham s-au confruntat pe Goodison Park.',\n",
    "    'Împotriva cui a debutat Rapid în noul an?' : 'Rapid debutează în noul an împotriva celor de la Poli Iași, luni seara, pe Giulești, de la 20:00.',\n",
    "    'Cu ce notă a fost notat Andrei Rațiu de Sofascore?' : 'Andrei Rațiu a fost notat cu 7,3 de Sofascore',\n",
    "    'Pe ce loc a terminat Unirea Urziceni Grupa G în sezonul 2009/10?' : 'În sezonul 2009/10, Unirea Urziceni a terminat Grupa G pe locul trei, cu opt puncte, deși adversare i-au fost FC Sevilla, VfB Stuttgart și Glasgow Rangers.',\n",
    "    'Cat va plăti Zenit St. Petersburg pentru Luiz Henrique?' : 'Zenit St. Petersburg va plăti 35 de milioane de euro pentru Luiz Henrique',\n",
    "    'Câte victorii a strâns Tag Heuer din postura de sponsor principal?' : 'Tag Heuer a strâns 230 de victorii',\n",
    "    'Ce ar vrea PSG?' : 'PSG ar vrea un fundaș, mai ales că se va despărți de Milan Skriniar',\n",
    "    'De ce spune Ralf Schumacher că Lewis Hamilton nu va avea impact la Ferrari?' : 'Ralf Schumacher spune că Lewis Hamilton nu va avea impactul așteptat la Ferrari, pentru că se află pe final de carieră.',\n",
    "    'Ce vârstă are Cahill, fostul antrenor al Simonei Halep?' : 'Cahill are 59 de ani.',\n",
    "    'Cum era pe vremuri jurnalismul?' : 'Pe vremuri, jurnalismul, inclusiv cel sportiv, a implicat să ai o conduită ireproșabilă în fața publicului',\n",
    "    'Ce record a stabilit Andreas Almgren?' : 'Andreas Almgren, a stabilit un nou record european la 10 km, parcurgând distanţa în 26 de minute şi 53 de secunde.',\n",
    "    'Ce naționalitate are Beatrice Chebet?' : 'Beatrice Chebet este de naționalitate kenyană.',\n",
    "    'Spune-mi despre Radu Drăgușin' : 'Radu Drăgușin este un fotbalist român care joacă pentru echipa Tottenham Hotspur. În timpul unui meci, recent, jucătorul Calvert-Lewin a trimis o lovitură cu cotul lui Drăgușin, accidentându-l.'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating answers with different temperatures or a fixed temperature to see the variety in the results obtained (BERTScoreF1) and the answers by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:14:27.625084Z",
     "iopub.status.busy": "2025-01-25T11:14:27.624747Z",
     "iopub.status.idle": "2025-01-25T11:14:27.630853Z",
     "shell.execute_reply": "2025-01-25T11:14:27.630036Z",
     "shell.execute_reply.started": "2025-01-25T11:14:27.625057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def max_bertscore_answers(golden_standard, temp, max_len):\n",
    "    \n",
    "    bertscore = load(\"bertscore\")\n",
    "    temp = [temp] if not isinstance(temp, list) else temp\n",
    "    \n",
    "    answers_max_f1 = []\n",
    "    max_f1 = []\n",
    "    prediction_scores_dictionary = {}\n",
    "    \n",
    "    for query_rag in golden_standard.keys():\n",
    "        \n",
    "        for t in temp:\n",
    "            predicted = rag_testing(query_rag, t, max_len)\n",
    "            \n",
    "            reference = golden_standard[query_rag]\n",
    "            \n",
    "            result = bertscore.compute(predictions = [predicted], references = [reference], lang = \"ro\")\n",
    "            if predicted not in prediction_scores_dictionary.keys():\n",
    "                prediction_scores_dictionary[predicted] = result['f1'][0]\n",
    "\n",
    "        answer_with_max_f1 = max(prediction_scores_dictionary, key = prediction_scores_dictionary.get)\n",
    "        answers_max_f1.append(answer_with_max_f1)\n",
    "        max_f1.append(prediction_scores_dictionary[answer_with_max_f1])\n",
    "        prediction_scores_dictionary = {}\n",
    "\n",
    "    return answers_max_f1, max_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the answers and the responses as a histogram to get a visual representation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T07:41:55.333846Z",
     "iopub.status.busy": "2025-01-25T07:41:55.333431Z",
     "iopub.status.idle": "2025-01-25T07:44:09.872351Z",
     "shell.execute_reply": "2025-01-25T07:44:09.871486Z",
     "shell.execute_reply.started": "2025-01-25T07:41:55.333820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_answers_histogram(answers, f1_scores, threshold):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(f1_scores, bins=10, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
    "    plt.axvline(x=threshold, color=\"red\", linestyle=\"--\", label=f\"Threshold ({threshold})\")\n",
    "\n",
    "    plt.title(\"Histogram of BERTScore F1 for LLM Outputs\", fontsize=14)\n",
    "    plt.xlabel(\"BERTScore F1\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "answers_max_f1, max_f1 = max_bertscore_answers(golden_standard, [0.1], 128)\n",
    "plot_answers_histogram(answers_max_f1, max_f1, 0.75)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6513163,
     "sourceId": 10523716,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
